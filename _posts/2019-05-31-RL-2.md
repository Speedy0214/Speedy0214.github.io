---
layout: post
title: "Markov Decision Processes"
categories: [Reinforcement Learning]
date: 2019-05-31
---

## [Markov Decision Processes](/assets/MDP.pdf)
---

### MDP
**Markov Property**:" the future is independent of the past given the present"

**Markov Process**:A Markov process is a memoryless random process, i.e. a sequence of random states S1, S2, ... with the **Markov property**.

![Example: Student Markov Chain](/assets/eg_mdp.jpg)

**Markov Reward Process**: a Markov Reword Process is a markov chain with values.

**Definition**
*A Markov Reword Process is a tuple $<S, P, R, \gamma>$*   
>$S$ is a finite set of states  
>$P$ is a state transition probability matrix, $P_{ss'} = P[S_{t+1} = s' | S_{t} = s]$  
>$R$ is a reward function, $R_{s} = E[R_{t+1}|S_t = s]$  
>$\gamma$ is a discount factor, $\gamma \in[0, 1]$


![Example: Student MRP](/assets/eg.MRP.jpg)

**RETURN(G means Goal)**  
>*Definition: The return $G_t$ is the total discounted reward from time-step t.*  
$$G_t = R_{t+1} + \gamma R_{t+2} +... = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$
>*g is random , g is just one sample from markov reward process*

**Value Function**  
>*The value function $v(s)$ gives the long-term value of state s*  
>*Definition: The state value function $v(s)$ of an MRP is the expected return starting from state s*  
$$v(s)=E[G_t |S_t =s]$$

**Bellman Equation for MRPs(foundation)**  
*The value function can be decomposed into two parts:*
>*immediate reward $R_{t+1}$*  
>*discounted value of successor state $\gamma v(S_{t+1})$*

$$\begin{align}
v(s) =& E[G_t |S_t =s] \\
=& E[R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+...|S_t=s]\\
=& E[R_{t+1}+Î³(R_{t+2}+\gamma R_{t+3}+...) |S_t =s] \\
=& E[R_{t+1}+\gamma G_{t+1} |S_t=s] \\
=& E[R_{t+1}+\gamma v(S_{t+1})|S_t =s]
\end{align} $$

$$v(s)=E[R_{t+1}+\gamma v(S_{t+1})|S_t =s]$$
![Bellman Equation Backup Diagram](/assets/BellmanEquationbackupdiagram1.jpg)
$$v(s)=R_s+\gamma P_{ss'}v(s')$$





---

<h2>Reference</h2>

<small>[www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf)</small>
