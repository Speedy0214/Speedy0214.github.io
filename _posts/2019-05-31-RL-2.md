---
layout: post
title: "Markov Decision Processes"
categories: [Reinforcement Learning]
date: 2019-05-31
---

## [Markov Decision Processes](/assets/MDP.pdf)
---
$$MP \rightarrow MRP \rightarrow MDP$$

### MP
**Markov Property**:" the future is independent of the past given the present"

**Markov Process**:A Markov process is a memoryless random process, i.e. a sequence of random states S1, S2, ... with the **Markov property**.
>Definition
>>A Markov Process (or Markov Chain) is a tuple $⟨S,P⟩$ $S$ is a (finite) set of states
$P$ is a state transition probability matrix, $P_{ss'} = P[S_{t+1}=s'|S_t=s]$


![Example: Student Markov Chain](/assets/eg_mdp.jpg)

**Markov Reward Process**: a Markov Reword Process is a markov chain with values.

**Definition**
>A Markov Reword Process is a tuple $<S, P, R, \gamma>$   
>>$S$ is a finite set of states  
>>$P$ is a state transition probability matrix, $P_{ss'} = P[S_{t+1} = s' | S_{t} = s]$  
>>$R$ is a reward function, $R_{s} = E[R_{t+1}|S_t = s]$  
>>$\gamma$ is a discount factor, $\gamma \in[0, 1]$


![Example: Student MRP](/assets/eg.MRP.jpg)

**RETURN(G means Goal)**  
>Definition: The return $G_t$ is the total discounted reward from time-step t.  
$$G_t = R_{t+1} + \gamma R_{t+2} +... = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$$
>>$G_t$ is random , G_t is just one sample from markov reward process

**Value Function(MRP)**  
>The value function $v(s)$ gives the long-term value of state s  
>>Definition: The state value function $v(s)$ of an MRP is the expected return starting from state s  
$$v(s)=E[G_t |S_t =s]$$

**Bellman Equation for MRPs(foundation)**  
>The value function can be decomposed into two parts:
>>immediate reward $R_{t+1}$  
>>discounted value of successor state $\gamma v(S_{t+1})$

>>$$\begin{align}
v(s) =& E[G_t |S_t =s] \\
=& E[R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+...|S_t=s]\\
=& E[R_{t+1}+γ(R_{t+2}+\gamma R_{t+3}+...) |S_t =s] \\
=& E[R_{t+1}+\gamma G_{t+1} |S_t=s] \\
=& E[R_{t+1}+\gamma v(S_{t+1})|S_t =s]
\end{align} $$


>>![Bellman Equation Backup Diagram](/assets/BellmanEquationbackupdiagram1.jpg)


**Bellman Equation in Matrix Form**
> The Bellman equation can be expressed concisely using matrices,
$$V = R + \gamma PV$$  
> where $V$ is a column vector with one entry per state  

$$
\begin{bmatrix}
v_{1}\\
\vdots\\
v_{n}\\
\end{bmatrix} = \begin{bmatrix}
R_{1}\\
\vdots\\
R_{n}\\
\end{bmatrix} + \begin{bmatrix}
P_{11} \cdots P_{1n}\\
\vdots \ddots \vdots\\
P_{n1}\cdots P_{nn}\\
\end{bmatrix}\begin{bmatrix}
v_{1}\\
\vdots\\
v_{n}\\
\end{bmatrix}
$$

**Solving the Bellman Equation**
> The Bellman equation is a linear equation
> It can be solved directly:  
$$\begin{align}
V =& R + \gamma PV \\
(I - \gamma P)V =& R \\
V =& (I-\gamma P)^{-1}R \\
\end{align}$$

>But
>> Computational complexity is $O(n^{3})$ for n states  
>> Direct solution only possible for samll MRPs  

>Iterative methods for large MRPs, e.g.
>> Dynamic programming  
>> Monte-Carlo Evaluation (MC)  
>> Temporal-Difference learning(TD)  

## Markov Decision Process
>A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.
Definition  

> A Markov Decision Process is a tuple $⟨S, \color{red}A, P, R, \gamma⟩$  
>>$S$ is a finite set of states  
>> $\color{red}A$ is a finite set of actions  
>> $P$ is a state transition probability matrix  
>> $P_{ss'}^{\color{red}a} =P[St+1=s'|S_t=s, A_t=a] $  
>> $R$ is a reward function, $R_s^{\color{red}a} = E[R_{t+1} | S_t = s,A_t = a]$  
>> $\gamma$ is a discount factor $\gamma \in [0, 1]$.

![student mdp eg](/assets/student_mdp.jpg)

*take actions to maximaze the reward*

To do that, first talk about formalize *makeing decisions*,
then define **Policy**
## Policies
>Definition  
>A policy $\pi$ is a distribution over actions given states,
$$\pi(a|s)=P[A_t =a|S_t =s]$$

>>A policy fully defines the behaviour of an agent  
>>MDP policies depend on the current state (not the history)  
>>i.e. Policies are stationary (time-independent), $A_t∼\pi(·|S_t),\forall t>0$

>Given an MDP $M = ⟨S,A,P,R,\gamma⟩$ and a policy $\pi$  
>The state sequence $S1, S2, ...$ is a Markov process $⟨S, P^{\pi}⟩$  
>The state and reward sequence $S1, R2, S2, ...$ is a Markov reward process $⟨S, P^{\pi}, R^{\pi}, \gamma⟩$
>>where  
$$P_{s,s'}^{\pi}=\sum_{a\in A} \pi(a|s)P_{ss'}^a\\
R_{s,s'}^{\pi}=\sum_{a\in A} \pi(a|s)R_{ss'}^a$$  


## Value Function(MDP)
>Definition  
>>The state-value function $V_{\pi}(s)$ of an MDP is theexpected return starting from state $s$, and then followingpolicy $\pi$  
$$ V_{\pi}(s)=E_{\pi}[G_t |S_t =s]$$

>Definition  
>>The action-value function $q_{\pi}(s,a)$ is the expected return
>>starting from state $s$, taking action $a$, and then following policy $\pi$  
$$ q_{\pi}(s, a)=E_{\pi}[G_t |A_t =a]$$  



## Bellman Expectation Equation(MDP)  
>The state-value function can again be decomposed into immediate reward plus discounted value of successor state,  
$$v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$
The action-value function can similarly be decomposed,  
$$q_{pi}(s,a)=E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t = s,A_t = a]$$  


## Bellman Expectation Equation for $V\pi$  
![bellman_equation_v](/assets/bellman_equation_v.jpg)  

## Bellman Expectation Equation for $q_\pi$  
![bellman_equation_q](/assets/bellman_equation_q.jpg)  

## Bellman Expectation Equation for $V_\pi$(2)  

![bellman_equation_v2](/assets/bellman_equation_v2.jpg)  

## Bellman Expectation Equation for $q_\pi$(2)  
![bellman_equation_q2](/assets/bellman_equation_q2.jpg)  




---

<h2>Reference</h2>

<small>[www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf)</small>
