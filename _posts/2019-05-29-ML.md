---
layout: post
title: "Regression"
categories: [Machine Learning]
date: 2019-05-29
---

## [Regression](/assets/Regression.pdf)

---

<h3>LinearRegression</h3>

- [sklean ref](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)

- [Reference Implementation](http://localhost:8888/notebooks/LR.ipynb)  


##### step 1 :Hypothesis
$$h(w)(x) = w_{0}1+w_{1}x_1+...+w_nx_n$$

##### step 2 : Evaluate Function
define *Loss Function*  
$$L(w) = \frac{1}{2m}\sum_{i=1}^m (h_{w}(x^i) -y^i)^2$$

##### step 3 : find a suitable w

*gradient*  *for every j*
$$ \frac{\partial L(w)}{\partial w_j} = \frac{1}{m}\sum_{i=1}^{m}[h_w(x^{i}) -y^{i}]x_j$$

*use gradient decent*

*for every j repeat unitl converge*

$$w_j = w_j -\alpha\frac{\partial L(w)}{\partial w_j}$$

```python

class LinearRegression(object):
    def __init__(self, ):
        pass


    def fit(self, X, y):
        """
          while not converge:
            w = w - alpha*grad
        """
        pass


    def loss(self, w, X, y):
        pass

    def _grad(self, w, X, y):
        pass

    def predict(self, X):
        pass
```










---
<h2>Reference</h2>

<small>[speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/Regression.pdf](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/Regression.pdf)</small>
